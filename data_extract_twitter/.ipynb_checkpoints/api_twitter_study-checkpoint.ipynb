{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from api_keys import BEARER_TOKEN\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções (Análise Exploratória)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para plotar bar plot com a contagem de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_count_words(text_column=None,\n",
    "                         label_column=None,\n",
    "                         name_class=None,\n",
    "                         dataframe=None,\n",
    "                         metric='SUM',\n",
    "                         top=50,return_df=True):\n",
    "    \n",
    "    corpus = dataframe[text_column].values\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    data_vect = vectorizer.fit_transform(corpus)\n",
    "    data_vect = data_vect.toarray()\n",
    "    \n",
    "    df_count_words =  pd.DataFrame({\n",
    "    \"WORDS\":vectorizer.get_feature_names(),\n",
    "    \"MEAN\":data_vect.mean(axis=0),\n",
    "    \"SUM\":data_vect.sum(axis=0),\n",
    "    \"STD\":data_vect.std(axis=0),\n",
    "    }) \n",
    "    \n",
    "    \n",
    "\n",
    "    if return_df:\n",
    "    \n",
    "        return df_count_words[[metric,'WORDS']].sort_values(by=[metric],ascending=False)[0:top]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        \n",
    "        ax = sns.barplot(x=metric, \n",
    "                 y=\"WORDS\", \n",
    "                 data=df_count_words[[metric,'WORDS']].sort_values(by=[metric],\n",
    "                                                                            ascending=False)[0:top])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para plotar bar plot com tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_tf_idf(text_column=None,\n",
    "                         label_column=None,\n",
    "                         name_class=None,\n",
    "                         dataframe=None,\n",
    "                         metric='SUM',\n",
    "                         top=50,return_df=True):\n",
    "    \n",
    "    corpus = dataframe[text_column].values\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    data_vect = vectorizer.fit_transform(corpus)\n",
    "    data_vect = data_vect.toarray()\n",
    "    \n",
    "    df_count_words =  pd.DataFrame({\n",
    "    \"WORDS\":vectorizer.get_feature_names(),\n",
    "    \"MEAN\":data_vect.mean(axis=0),\n",
    "    \"SUM\":data_vect.sum(axis=0),\n",
    "    \"STD\":data_vect.std(axis=0),\n",
    "    \"MAX\":data_vect.std(axis=0)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "\n",
    "    if return_df:\n",
    "    \n",
    "        return df_count_words[[metric,'WORDS']].sort_values(by=[metric],ascending=False)[0:top]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        \n",
    "        ax = sns.barplot(x=metric, \n",
    "                 y=\"WORDS\", \n",
    "                 data=df_count_words[[metric,'WORDS']].sort_values(by=[metric],\n",
    "                                                                            ascending=False)[0:top])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para contagem de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_words(text):\n",
    "\n",
    "    quantity_of_words = text.split(\" \")\n",
    "\n",
    "    quantity_of_words = [i for i in quantity_of_words if i!=\"\"]\n",
    "\n",
    "    quantity_of_words = len(quantity_of_words)\n",
    "\n",
    "    return quantity_of_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para contagem de diferentes tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_diferent_words(text):\n",
    "\n",
    "    quantity_of_diferent_words = text.split(\" \")\n",
    "\n",
    "    quantity_of_diferent_words = [i for i in quantity_of_diferent_words if i!=\"\"]\n",
    "\n",
    "    quantity_of_diferent_words = set(quantity_of_diferent_words)\n",
    "\n",
    "    quantity_of_diferent_words = list(quantity_of_diferent_words)\n",
    "\n",
    "    quantity_of_diferent_words = len(quantity_of_diferent_words)\n",
    "\n",
    "    return quantity_of_diferent_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para criar textos sem repetição de palavras para ser utilizado na análise exploratória "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_no_repeat_words(text):\n",
    "\n",
    "    text_with_no_repeat_words = text.split(\" \")\n",
    "\n",
    "    text_with_no_repeat_words = [i for i in text_with_no_repeat_words if i!=\"\"]\n",
    "\n",
    "    text_with_no_repeat_words = set(text_with_no_repeat_words)\n",
    "\n",
    "    text_with_no_repeat_words = list(text_with_no_repeat_words)\n",
    "\n",
    "    text_with_no_repeat_words = \" \".join(text_with_no_repeat_words)\n",
    "\n",
    "    return text_with_no_repeat_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para o pré-processamento do texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \n",
    "    nltk_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "    collection_text = [ {\"text\" : text}]\n",
    "    text = pd.DataFrame(collection_text)\n",
    "\n",
    "    text['text'] = text['text'].astype('str')\n",
    "    text['text'] = text['text'].str.lower()\n",
    "    text['text'] = text['text'].str.replace('\\n',' ')\n",
    "    text['text'] = text['text'].str.replace('\\r',' ')\n",
    "    text['text'] = text['text'].apply(lambda x: norm('NFKD', x).encode('ascii', 'ignore').decode())\n",
    "    text['text'] = text['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
    "    text['text'] = text['text'].apply(lambda x: re.sub(r'\\s+',' ',x))\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(nltk_stopwords))\n",
    "    text['text'] = text['text'].str.replace(pat,'')\n",
    "    text = text['text'].values[0]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções (Extração de Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "\n",
    "\n",
    "def create_url(query = \"@globoplay -is:retweet\",until_id=None):\n",
    "    \n",
    "    #query = \"@BBB -is:retweet\"\n",
    "    #\"from:twitterdev -is:retweet\"\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    \n",
    "    if until_id:\n",
    "        \n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=100&until_id={}\".format(\n",
    "            query,until_id\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=100\".format(\n",
    "            query\n",
    "        )\n",
    "            \n",
    "    return url\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def extract_100_tweets(query = \"@BBB -is:retweet\",until_id=None):\n",
    "    bearer_token = BEARER_TOKEN\n",
    "    url = create_url(query,until_id)\n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "    data_tweets = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "    return json_response\n",
    "\n",
    "def extract_many_tweets(qnt_cycle=10,folder=\"data_tweets\",start_from_id=None,query=\"@BBB\"):\n",
    "    \n",
    "    \n",
    "    oldest_id = None\n",
    "    \n",
    "    for i in tqdm(range(qnt_cycle)):\n",
    "    \n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            #extract the 100 tweets first\n",
    "            \n",
    "            if start_from_id:\n",
    "        \n",
    "                data_tweets = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=None)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                data_tweets = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=start_from_id)\n",
    "                \n",
    "            \n",
    "            df_data_tweets_temp = pd.DataFrame(data_tweets[\"data\"])\n",
    "            \n",
    "            #get the current date\n",
    "            \n",
    "            date_extraction = datetime.now()\n",
    "            \n",
    "            df_data_tweets_temp[\"date_extraction\"] = date_extraction \n",
    "            \n",
    "            \n",
    "            oldest_id = data_tweets['meta']['oldest_id']\n",
    "            \n",
    "            oldest_date = date_extraction\n",
    "            \n",
    "            df_data_tweets = df_data_tweets_temp.copy()\n",
    "            \n",
    "            # name file\n",
    "            \n",
    "            date_extraction_str = str(date_extraction).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "            \n",
    "            name_file = \"./{}/persist_tweets_{}_{}.csv\".format(folder,date_extraction_str,date_extraction_str)\n",
    "            \n",
    "            # persist base\n",
    "            \n",
    "            df_data_tweets.to_csv(name_file,sep=\",\")\n",
    "            \n",
    "    \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            #extract more 100 tweets older\n",
    "            \n",
    "            data_tweets_temp = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=oldest_id)\n",
    "            \n",
    "            df_data_tweets_temp = pd.DataFrame(data_tweets_temp[\"data\"])\n",
    "            \n",
    "            \n",
    "            #get the current date\n",
    "            \n",
    "            \n",
    "            date_extraction = datetime.now()\n",
    "            \n",
    "            df_data_tweets_temp[\"date_extraction\"] = date_extraction \n",
    "            \n",
    "            oldest_id = data_tweets_temp['meta']['oldest_id']\n",
    "            \n",
    "            df_data_tweets = pd.concat([df_data_tweets,df_data_tweets_temp.copy()])\n",
    "            \n",
    "            date_extraction = datetime.now()\n",
    "            \n",
    "            df_data_tweets.reset_index(inplace=True,drop=True)\n",
    "            \n",
    "            \n",
    "            # remove old files\n",
    "            \n",
    "            os.remove(name_file)\n",
    "            \n",
    "            \n",
    "            # name file\n",
    "            \n",
    "            oldest_date_str = str(oldest_date).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "            \n",
    "            date_extraction_str = str(date_extraction).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "            \n",
    "            \n",
    "            name_file = \"./{}/persist_tweets_{}_{}.csv\".format(folder,oldest_date_str,date_extraction_str)\n",
    "            \n",
    "            # persist base\n",
    "            \n",
    "            df_data_tweets.to_csv(name_file.format(folder),sep=\",\")\n",
    "            \n",
    "            \n",
    "\n",
    "    return df_data_tweets\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "data_tweets_final = extract_many_tweets(qnt_cycle=10,folder=\"data_tweets\",query=\"fgv\")#,start_from_id=\"1367600965277384706\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date_extraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1383611297481170947</td>\n",
       "      <td>@davidgokhshtein Hathor blockchain, huge proje...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1383602944214331401</td>\n",
       "      <td>#انتحاااااار_شاب_بسبب_التتتتتتنمر\\nآصال\\nfGV</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1383599192581443584</td>\n",
       "      <td>@vinigilmore @najuleme NA FGV NÃO NA USP, MAS ...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1383595914057568256</td>\n",
       "      <td>@BolsonaroSP https://t.co/Y4YHMuRDyz.</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1383595304633606145</td>\n",
       "      <td>@TheInterceptBr @LucasPRezende Governo da verg...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1383595056406294532</td>\n",
       "      <td>@TheInterceptBr https://t.co/Y4YHMuRDyz.</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1383594640587120646</td>\n",
       "      <td>@JovemPanNews @jrguzzofatos https://t.co/Y4YHM...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1383586112338227209</td>\n",
       "      <td>@Oxlein @fgv_au @manaral90 @SaudiNews50 🤣🤣🤣</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1383585888270049280</td>\n",
       "      <td>@IMohammed2005 @fgv_au @manaral90 @SaudiNews50...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1383585849305038850</td>\n",
       "      <td>@manaral90 @fgv_au @Oxlein @SaudiNews50 😂✌🏻</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1383585752286515205</td>\n",
       "      <td>@IMohammed2005 @fgv_au @Oxlein @SaudiNews50 هه...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1383585666747887619</td>\n",
       "      <td>@fgv_au @manaral90 @Oxlein @SaudiNews50 يعني ن...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1383585553447198722</td>\n",
       "      <td>It’s a Squishy world and we have proof…(58 Pho...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1383583293426126849</td>\n",
       "      <td>@albertocalmeida Cadê a Gabi da FGV?</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1383581126602592258</td>\n",
       "      <td>to conversando com um menino q eh da turma da ...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1383577915829280782</td>\n",
       "      <td>@sergioalvesjuni Nem os nerd da fgv explica</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1383565050917523457</td>\n",
       "      <td>O Kingdom virou a gincana da FGV</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1383555468933488646</td>\n",
       "      <td>@BrunoCarazza Tentando acabar o último capítul...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1383549030123274240</td>\n",
       "      <td>@BobjeffHD Qdo meu pai foi assistir a minha fo...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1383547818053279747</td>\n",
       "      <td>@tatinhozen Lava jato do Deltan e do Moro, a m...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1383544883869208578</td>\n",
       "      <td>caralho que SAUDADE da FGV</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1383537325003534339</td>\n",
       "      <td>FGV faça uma prova dboa para amanhã prf 🙏🏻</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1383533109790924802</td>\n",
       "      <td>Segundo um estudo da FGV 50% das mulheres são ...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1383527455240036353</td>\n",
       "      <td>@larassmrqz @samsilvabitt Eh verdade, mas isso...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1383527286578630657</td>\n",
       "      <td>@larassmrqz Depende da universidade. FGV, PUC,...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1383526451408867329</td>\n",
       "      <td>#CursoGratuito | Qual é a importância da análi...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1383520497443344384</td>\n",
       "      <td>FGV abre inscrições para o mestrado executivo....</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1383518941780201474</td>\n",
       "      <td>@fgv_au @moathG29 وش نسوي هذا الحال 💔</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1383517027663450115</td>\n",
       "      <td>https://t.co/MnaTxOoJRQ</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1383515911978508295</td>\n",
       "      <td>@Estadao Volta passagem tenebrosa: \\n\\nhttps:/...</td>\n",
       "      <td>2021-04-17 23:45:45.394875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0   1383611297481170947  @davidgokhshtein Hathor blockchain, huge proje...   \n",
       "1   1383602944214331401       #انتحاااااار_شاب_بسبب_التتتتتتنمر\\nآصال\\nfGV   \n",
       "2   1383599192581443584  @vinigilmore @najuleme NA FGV NÃO NA USP, MAS ...   \n",
       "3   1383595914057568256              @BolsonaroSP https://t.co/Y4YHMuRDyz.   \n",
       "4   1383595304633606145  @TheInterceptBr @LucasPRezende Governo da verg...   \n",
       "5   1383595056406294532           @TheInterceptBr https://t.co/Y4YHMuRDyz.   \n",
       "6   1383594640587120646  @JovemPanNews @jrguzzofatos https://t.co/Y4YHM...   \n",
       "7   1383586112338227209        @Oxlein @fgv_au @manaral90 @SaudiNews50 🤣🤣🤣   \n",
       "8   1383585888270049280  @IMohammed2005 @fgv_au @manaral90 @SaudiNews50...   \n",
       "9   1383585849305038850        @manaral90 @fgv_au @Oxlein @SaudiNews50 😂✌🏻   \n",
       "10  1383585752286515205  @IMohammed2005 @fgv_au @Oxlein @SaudiNews50 هه...   \n",
       "11  1383585666747887619  @fgv_au @manaral90 @Oxlein @SaudiNews50 يعني ن...   \n",
       "12  1383585553447198722  It’s a Squishy world and we have proof…(58 Pho...   \n",
       "13  1383583293426126849               @albertocalmeida Cadê a Gabi da FGV?   \n",
       "14  1383581126602592258  to conversando com um menino q eh da turma da ...   \n",
       "15  1383577915829280782        @sergioalvesjuni Nem os nerd da fgv explica   \n",
       "16  1383565050917523457                   O Kingdom virou a gincana da FGV   \n",
       "17  1383555468933488646  @BrunoCarazza Tentando acabar o último capítul...   \n",
       "18  1383549030123274240  @BobjeffHD Qdo meu pai foi assistir a minha fo...   \n",
       "19  1383547818053279747  @tatinhozen Lava jato do Deltan e do Moro, a m...   \n",
       "20  1383544883869208578                         caralho que SAUDADE da FGV   \n",
       "21  1383537325003534339         FGV faça uma prova dboa para amanhã prf 🙏🏻   \n",
       "22  1383533109790924802  Segundo um estudo da FGV 50% das mulheres são ...   \n",
       "23  1383527455240036353  @larassmrqz @samsilvabitt Eh verdade, mas isso...   \n",
       "24  1383527286578630657  @larassmrqz Depende da universidade. FGV, PUC,...   \n",
       "25  1383526451408867329  #CursoGratuito | Qual é a importância da análi...   \n",
       "26  1383520497443344384  FGV abre inscrições para o mestrado executivo....   \n",
       "27  1383518941780201474              @fgv_au @moathG29 وش نسوي هذا الحال 💔   \n",
       "28  1383517027663450115                            https://t.co/MnaTxOoJRQ   \n",
       "29  1383515911978508295  @Estadao Volta passagem tenebrosa: \\n\\nhttps:/...   \n",
       "\n",
       "              date_extraction  \n",
       "0  2021-04-17 23:45:45.394875  \n",
       "1  2021-04-17 23:45:45.394875  \n",
       "2  2021-04-17 23:45:45.394875  \n",
       "3  2021-04-17 23:45:45.394875  \n",
       "4  2021-04-17 23:45:45.394875  \n",
       "5  2021-04-17 23:45:45.394875  \n",
       "6  2021-04-17 23:45:45.394875  \n",
       "7  2021-04-17 23:45:45.394875  \n",
       "8  2021-04-17 23:45:45.394875  \n",
       "9  2021-04-17 23:45:45.394875  \n",
       "10 2021-04-17 23:45:45.394875  \n",
       "11 2021-04-17 23:45:45.394875  \n",
       "12 2021-04-17 23:45:45.394875  \n",
       "13 2021-04-17 23:45:45.394875  \n",
       "14 2021-04-17 23:45:45.394875  \n",
       "15 2021-04-17 23:45:45.394875  \n",
       "16 2021-04-17 23:45:45.394875  \n",
       "17 2021-04-17 23:45:45.394875  \n",
       "18 2021-04-17 23:45:45.394875  \n",
       "19 2021-04-17 23:45:45.394875  \n",
       "20 2021-04-17 23:45:45.394875  \n",
       "21 2021-04-17 23:45:45.394875  \n",
       "22 2021-04-17 23:45:45.394875  \n",
       "23 2021-04-17 23:45:45.394875  \n",
       "24 2021-04-17 23:45:45.394875  \n",
       "25 2021-04-17 23:45:45.394875  \n",
       "26 2021-04-17 23:45:45.394875  \n",
       "27 2021-04-17 23:45:45.394875  \n",
       "28 2021-04-17 23:45:45.394875  \n",
       "29 2021-04-17 23:45:45.394875  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets_final.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_calc_histogram(x,initial_interval, final_interval,n_bins,indice=False):\n",
    "\n",
    "    interval = np.linspace(initial_interval, final_interval, num=n_bins)\n",
    "\n",
    "    for j,i in enumerate(interval):\n",
    "\n",
    "\n",
    "        if i == interval[len(interval)-1]:\n",
    "\n",
    "            if x>=i:\n",
    "\n",
    "                \n",
    "                if indice:\n",
    "\n",
    "                    return j\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    return \"{}<\".format(x)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            if x>=i and x<interval[j+1]:\n",
    "\n",
    "                inicial = round(i, 1)\n",
    "\n",
    "                final = round(interval[j+1],1)\n",
    "\n",
    "            \n",
    "                if indice:\n",
    "\n",
    "                    return j\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    return \"[{},{})\".format(inicial,final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de uma coluna com os textos sem repetição de palavras para ser utilizado na análise exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['text_unique_words'] = data_tweets_final['text'].apply(lambda x: convert_text_to_no_repeat_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo Número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['number_tokens'] = data_tweets_final['text'].apply(lambda x: calculate_number_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo Número de diferentes tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['number_diferent_tokens'] = data_tweets_final['text'].apply(lambda x: calculate_number_diferent_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máximo número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = data_tweets_final[\"number_tokens\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mínimo número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = data_tweets_final[\"number_tokens\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados para o histograma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58<\n"
     ]
    }
   ],
   "source": [
    "data_tweets_final['bins'] = data_tweets_final['number_tokens'].apply(lambda x: function_to_calc_histogram(x,initial_interval = min_count, final_interval = max_count,n_bins = 10,indice=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58<\n"
     ]
    }
   ],
   "source": [
    "data_tweets_final['indices_bins'] = data_tweets_final['number_tokens'].apply(lambda x: function_to_calc_histogram(x,initial_interval = min_count, final_interval = max_count,n_bins = 10,indice=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram = data_tweets_final.groupby([\"bins\",\"indices_bins\"]).sum().sort_values(by=[\"indices_bins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bins</th>\n",
       "      <th>indices_bins</th>\n",
       "      <th>number_tokens</th>\n",
       "      <th>number_diferent_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0,7.3)</td>\n",
       "      <td>0</td>\n",
       "      <td>630</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[7.3,13.7)</td>\n",
       "      <td>1</td>\n",
       "      <td>2271</td>\n",
       "      <td>2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[13.7,20.0)</td>\n",
       "      <td>2</td>\n",
       "      <td>2931</td>\n",
       "      <td>2819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[20.0,26.3)</td>\n",
       "      <td>3</td>\n",
       "      <td>2492</td>\n",
       "      <td>2357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[26.3,32.7)</td>\n",
       "      <td>4</td>\n",
       "      <td>2329</td>\n",
       "      <td>2183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[32.7,39.0)</td>\n",
       "      <td>5</td>\n",
       "      <td>2643</td>\n",
       "      <td>2381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[39.0,45.3)</td>\n",
       "      <td>6</td>\n",
       "      <td>3954</td>\n",
       "      <td>3485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[45.3,51.7)</td>\n",
       "      <td>7</td>\n",
       "      <td>1893</td>\n",
       "      <td>1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[51.7,58.0)</td>\n",
       "      <td>8</td>\n",
       "      <td>2074</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>58&lt;</td>\n",
       "      <td>9</td>\n",
       "      <td>58</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          bins  indices_bins  number_tokens  number_diferent_tokens\n",
       "0    [1.0,7.3)             0            630                     629\n",
       "1   [7.3,13.7)             1           2271                    2232\n",
       "2  [13.7,20.0)             2           2931                    2819\n",
       "3  [20.0,26.3)             3           2492                    2357\n",
       "4  [26.3,32.7)             4           2329                    2183\n",
       "5  [32.7,39.0)             5           2643                    2381\n",
       "6  [39.0,45.3)             6           3954                    3485\n",
       "7  [45.3,51.7)             7           1893                    1638\n",
       "8  [51.7,58.0)             8           2074                    1851\n",
       "9          58<             9             58                      52"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1.0,7.3)',\n",
       " '[7.3,13.7)',\n",
       " '[13.7,20.0)',\n",
       " '[20.0,26.3)',\n",
       " '[26.3,32.7)',\n",
       " '[32.7,39.0)',\n",
       " '[39.0,45.3)',\n",
       " '[45.3,51.7)',\n",
       " '[51.7,58.0)',\n",
       " '58<']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_histogram[\"bins\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[630, 2271, 2931, 2492, 2329, 2643, 3954, 1893, 2074, 58]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_histogram[\"number_tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_mean = plot_bar_count_words(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='MEAN',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.02, 0.348, 0.299, 0.299, 0.298, 0.264, 0.246, 0.238, 0.194, 0.194]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report_mean[\"MEAN\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boninho',\n",
       " 'bbb21',\n",
       " 'https',\n",
       " 'co',\n",
       " 'que',\n",
       " 'juliette',\n",
       " 'de',\n",
       " 'bbb',\n",
       " 'camilla',\n",
       " 'fiuk']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report_mean[\"WORDS\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 SUM docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum = plot_bar_count_words(text_column='text_unique_words',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='SUM',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum[\"P_DOCS\"] =  df_report_sum[\"SUM\"]/len(data_tweets_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUM</th>\n",
       "      <th>WORDS</th>\n",
       "      <th>P_DOCS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>799</td>\n",
       "      <td>fgv</td>\n",
       "      <td>0.804632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>704</td>\n",
       "      <td>co</td>\n",
       "      <td>0.708963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>704</td>\n",
       "      <td>https</td>\n",
       "      <td>0.708963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>490</td>\n",
       "      <td>de</td>\n",
       "      <td>0.493454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>389</td>\n",
       "      <td>da</td>\n",
       "      <td>0.391742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>318</td>\n",
       "      <td>em</td>\n",
       "      <td>0.320242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4481</th>\n",
       "      <td>233</td>\n",
       "      <td>que</td>\n",
       "      <td>0.234642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>197</td>\n",
       "      <td>do</td>\n",
       "      <td>0.198389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3701</th>\n",
       "      <td>152</td>\n",
       "      <td>na</td>\n",
       "      <td>0.153072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>130</td>\n",
       "      <td>com</td>\n",
       "      <td>0.130916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SUM  WORDS    P_DOCS\n",
       "2240  799    fgv  0.804632\n",
       "1139  704     co  0.708963\n",
       "2664  704  https  0.708963\n",
       "1479  490     de  0.493454\n",
       "1445  389     da  0.391742\n",
       "1858  318     em  0.320242\n",
       "4481  233    que  0.234642\n",
       "1720  197     do  0.198389\n",
       "3701  152     na  0.153072\n",
       "1167  130    com  0.130916"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum_docs = plot_bar_count_words(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='SUM',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 MEAN TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_mean = plot_bar_tf_idf(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='MEAN',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 MAX TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_max = plot_bar_tf_idf(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='MAX',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
