{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/julio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from api_keys import BEARER_TOKEN\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize as norm\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str).encode('ASCII', 'ignore')\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicodedata.normalize('NFKD', \"mãe\").encode('ASCII', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm('NFKD', \"mãe\").encode('ascii', 'ignore').decode().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"https:4//t.co/lmvoS5uwFd poxa globoplay...  https:3//t.co/lmvoS5uwFd   https:2//t.co/lmvoS5uwFd https:1//t.co/lmvoS5uwFd   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regex_remove_https = 'https([a-zA-Zà-úÀ-Ú0-9]|[-()\\#/@;:<>{}`+=~|.!?,])+'\n",
    "#text_without_https = re.sub(r\"\\s{}\\s|\\s{}$|^{}\\s\".format(regex_remove_https,regex_remove_https,regex_remove_https),\" \",text)\n",
    "#print(\"\\n\")\n",
    "#print(\"\\s{}\\s|\\s{}$|^{}\\s\".format(regex_remove_https,regex_remove_https,regex_remove_https))\n",
    "#print(\"\\n\")\n",
    "\n",
    "text_without_https = re.sub(r\"(\\s|^){0}(\\s{0})*($|\\s)\".format(regex_remove_https),\" \",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_without_https"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re.search(r\"\\s{}\\s|\\s{}$|^{}\\s\".format(regex_remove_https,regex_remove_https,regex_remove_https),text_without_https):\n",
    "    \n",
    "    print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"\\s{}\\s|\\s{}$|^{}\\s\".format(regex_remove_https,regex_remove_https,regex_remove_https),text,re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções (Análise Exploratória)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para plotar bar plot com a contagem de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_count_words(text_column=None,\n",
    "                         label_column=None,\n",
    "                         name_class=None,\n",
    "                         dataframe=None,\n",
    "                         metric='SUM',\n",
    "                         top=50,return_df=True):\n",
    "    \n",
    "    corpus = dataframe[text_column].values\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    data_vect = vectorizer.fit_transform(corpus)\n",
    "    data_vect = data_vect.toarray()\n",
    "    \n",
    "    df_count_words =  pd.DataFrame({\n",
    "    \"WORDS\":vectorizer.get_feature_names(),\n",
    "    \"MEAN\":data_vect.mean(axis=0),\n",
    "    \"SUM\":data_vect.sum(axis=0),\n",
    "    \"STD\":data_vect.std(axis=0),\n",
    "    }) \n",
    "    \n",
    "    \n",
    "\n",
    "    if return_df:\n",
    "    \n",
    "        return df_count_words[[metric,'WORDS']].sort_values(by=[metric],ascending=False)[0:top]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        \n",
    "        ax = sns.barplot(x=metric, \n",
    "                 y=\"WORDS\", \n",
    "                 data=df_count_words[[metric,'WORDS']].sort_values(by=[metric],\n",
    "                                                                            ascending=False)[0:top])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para plotar bar plot com tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_tf_idf(text_column=None,\n",
    "                         label_column=None,\n",
    "                         name_class=None,\n",
    "                         dataframe=None,\n",
    "                         metric='SUM',\n",
    "                         top=50,return_df=True):\n",
    "    \n",
    "    corpus = dataframe[text_column].values\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    data_vect = vectorizer.fit_transform(corpus)\n",
    "    data_vect = data_vect.toarray()\n",
    "    \n",
    "    df_count_words =  pd.DataFrame({\n",
    "    \"WORDS\":vectorizer.get_feature_names(),\n",
    "    \"MEAN\":data_vect.mean(axis=0),\n",
    "    \"SUM\":data_vect.sum(axis=0),\n",
    "    \"STD\":data_vect.std(axis=0),\n",
    "    \"MAX\":data_vect.std(axis=0)\n",
    "    }) \n",
    "    \n",
    "    \n",
    "\n",
    "    if return_df:\n",
    "    \n",
    "        return df_count_words[[metric,'WORDS']].sort_values(by=[metric],ascending=False)[0:top]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        fig = plt.figure(figsize=(15,10))\n",
    "        \n",
    "        ax = sns.barplot(x=metric, \n",
    "                 y=\"WORDS\", \n",
    "                 data=df_count_words[[metric,'WORDS']].sort_values(by=[metric],\n",
    "                                                                            ascending=False)[0:top])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para contagem de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_words(text):\n",
    "\n",
    "    quantity_of_words = text.split(\" \")\n",
    "\n",
    "    quantity_of_words = [i for i in quantity_of_words if i!=\"\"]\n",
    "\n",
    "    quantity_of_words = len(quantity_of_words)\n",
    "\n",
    "    return quantity_of_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para contagem de diferentes tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_number_diferent_words(text):\n",
    "\n",
    "    quantity_of_diferent_words = text.split(\" \")\n",
    "\n",
    "    quantity_of_diferent_words = [i for i in quantity_of_diferent_words if i!=\"\"]\n",
    "\n",
    "    quantity_of_diferent_words = set(quantity_of_diferent_words)\n",
    "\n",
    "    quantity_of_diferent_words = list(quantity_of_diferent_words)\n",
    "\n",
    "    quantity_of_diferent_words = len(quantity_of_diferent_words)\n",
    "\n",
    "    return quantity_of_diferent_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para criar textos sem repetição de palavras para ser utilizado na análise exploratória "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_no_repeat_words(text):\n",
    "\n",
    "    text_with_no_repeat_words = text.split(\" \")\n",
    "\n",
    "    text_with_no_repeat_words = [i for i in text_with_no_repeat_words if i!=\"\"]\n",
    "\n",
    "    text_with_no_repeat_words = set(text_with_no_repeat_words)\n",
    "\n",
    "    text_with_no_repeat_words = list(text_with_no_repeat_words)\n",
    "\n",
    "    text_with_no_repeat_words = \" \".join(text_with_no_repeat_words)\n",
    "\n",
    "    return text_with_no_repeat_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para o pré-processamento do texto "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \n",
    "    nltk_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "    collection_text = [ {\"text\" : text}]\n",
    "    text = pd.DataFrame(collection_text)\n",
    "\n",
    "    text['text'] = text['text'].astype('str')\n",
    "    text['text'] = text['text'].str.lower()\n",
    "    text['text'] = text['text'].str.replace('\\n',' ')\n",
    "    text['text'] = text['text'].str.replace('\\r',' ')\n",
    "    text['text'] = text['text'].apply(lambda x: norm('NFKD', x).encode('ascii', 'ignore').decode())\n",
    "    text['text'] = text['text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
    "    text['text'] = text['text'].apply(lambda x: re.sub(r'\\s+',' ',x))\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(nltk_stopwords))\n",
    "    text['text'] = text['text'].str.replace(pat,'')\n",
    "    text = text['text'].values[0]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para limpeza dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text,stop_words_domain =None):\n",
    "    \n",
    "    nltk_stopwords = stopwords.words('portuguese')\n",
    "    regex_stop_words = '|'.join(nltk_stopwords)\n",
    "    text = re.sub(r\"\\shttps([a-zA-Zà-úÀ-Ú0-9]|[-()\\\"#/@;:<>{}`+=~|.!?,])+$|^https([a-zA-Zà-úÀ-Ú0-9]|[-()\\\"#/@;:<>{}`+=~|.!?,])+\\s|\\shttps([a-zA-Zà-úÀ-Ú0-9]|[-()\\\"#/@;:<>{}`+=~|.!?,])+\\s\",\" \",text)\n",
    "    text = re.sub(r\"[^a-zA-ZÀ-Úà-ú]+\",\" \",text)\n",
    "    text = re.sub(r\"\\s({})\\s|\\s({})$|^({})\\s\".format(regex_stop_words,regex_stop_words,regex_stop_words),\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \" uhahua,não,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções (Extração de Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "\n",
    "\n",
    "def create_url(query = \"@globoplay -is:retweet\",until_id=None):\n",
    "    \n",
    "    #query = \"@BBB -is:retweet\"\n",
    "    #\"from:twitterdev -is:retweet\"\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    \n",
    "    if until_id:\n",
    "        \n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=10&until_id={}&tweet.fields=author_id,created_at\".format(\n",
    "            query,until_id\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=10&tweet.fields=author_id,created_at\".format(\n",
    "            query\n",
    "        )\n",
    "            \n",
    "    return url\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def extract_100_tweets(query = \"@BBB -is:retweet\",until_id=None):\n",
    "    bearer_token = BEARER_TOKEN\n",
    "    url = create_url(query,until_id)\n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "    data_tweets = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(json_response)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return json_response\n",
    "\n",
    "def extract_many_tweets(qnt_cycle=10,folder=\"data_tweets\",until_id=None,query=\"@BBB\",start_from_id=None):\n",
    "    \n",
    "    \n",
    "    oldest_id = None\n",
    "    \n",
    "    newest_id = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in tqdm(range(qnt_cycle)):\n",
    "    \n",
    "        \n",
    "        if i == 0:\n",
    "            \n",
    "            #extract the 100 tweets first\n",
    "            \n",
    "            if not start_from_id:\n",
    "        \n",
    "                data_tweets = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=None)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                data_tweets = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=until_id)\n",
    "                \n",
    "            \n",
    "            df_data_tweets_temp = pd.DataFrame(data_tweets[\"data\"])\n",
    "            \n",
    "            #get the current date\n",
    "            \n",
    "            date_extraction = datetime.now()\n",
    "            \n",
    "            df_data_tweets_temp[\"date_extraction\"] = date_extraction \n",
    "            \n",
    "            \n",
    "            oldest_id = data_tweets['meta']['oldest_id']\n",
    "            \n",
    "            newest_id = data_tweets['meta']['newest_id']\n",
    "            \n",
    "            oldest_date = date_extraction\n",
    "            \n",
    "            df_data_tweets = df_data_tweets_temp.copy()\n",
    "            \n",
    "            # name file\n",
    "            \n",
    "            date_extraction_str = str(date_extraction).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "            \n",
    "            name_file = \"./{}/persist_tweets_{}_{}.csv\".format(folder,date_extraction_str,date_extraction_str)\n",
    "            \n",
    "            # persist base\n",
    "            \n",
    "            df_data_tweets.to_csv(name_file,sep=\",\")\n",
    "            \n",
    "    \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            #extract more 100 tweets older\n",
    "            \n",
    "            data_tweets_temp = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=oldest_id)\n",
    "            \n",
    "            df_data_tweets_temp = pd.DataFrame(data_tweets_temp[\"data\"])\n",
    "            \n",
    "            \n",
    "            #get the current date\n",
    "            \n",
    "            \n",
    "            date_extraction = datetime.now()\n",
    "            \n",
    "            df_data_tweets_temp[\"date_extraction\"] = date_extraction \n",
    "            \n",
    "            oldest_id = data_tweets_temp['meta']['oldest_id']\n",
    "            \n",
    "            df_data_tweets = pd.concat([df_data_tweets,df_data_tweets_temp.copy()])\n",
    "            \n",
    "            date_extraction = datetime.now()\n",
    "            \n",
    "            df_data_tweets.reset_index(inplace=True,drop=True)\n",
    "            \n",
    "            \n",
    "            # remove old files\n",
    "            \n",
    "            os.remove(name_file)\n",
    "            \n",
    "            \n",
    "            # name file\n",
    "            \n",
    "            oldest_date_str = str(oldest_date).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "            \n",
    "            date_extraction_str = str(date_extraction).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "            \n",
    "            \n",
    "            name_file = \"./{}/persist_tweets_{}_{}.csv\".format(folder,oldest_date_str,date_extraction_str)\n",
    "            \n",
    "            # persist base\n",
    "            \n",
    "            df_data_tweets.to_csv(name_file.format(folder),sep=\",\")\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    return df_data_tweets\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def create_url(query = None,until_id=None,since_id=None):\n",
    "    \n",
    "    #query = \"@BBB -is:retweet\"\n",
    "    #\"from:twitterdev -is:retweet\"\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    \n",
    "    if until_id:\n",
    "        \n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=10&tweet.fields=author_id,created_at&until_id={}&\".format(\n",
    "            query,until_id\n",
    "        )\n",
    "\n",
    "    elif since_id:\n",
    "\n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=10&tweet.fields=author_id,created_at&since_id={}&\".format(\n",
    "            query,since_id\n",
    "        )\n",
    "\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results=10&tweet.fields=author_id,created_at\".format(\n",
    "            query\n",
    "        )\n",
    "            \n",
    "    return url\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers):\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def extract_100_tweets(query = None,until_id=None,since_id=None,key_twitter = None):\n",
    "    bearer_token = key_twitter\n",
    "    \n",
    "    if not until_id:\n",
    "        \n",
    "        url = create_url(query,until_id = until_id)\n",
    "        \n",
    "    elif not since_id:\n",
    "        \n",
    "        url = create_url(query,since_id = since_id)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        url = create_url(query = query)\n",
    "    \n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "    data_tweets = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "    return json_response\n",
    "\n",
    "def extract_many_tweets(qnt_cycle=10,\n",
    "                        folder=\"data_tweets\",\n",
    "                        until_id=None,\n",
    "                        since_id=None,\n",
    "                        query=\"@BBB\",\n",
    "                        bearer_token = None,\n",
    "                        df_concat_path=None):\n",
    "\n",
    "\n",
    "    if since_id:\n",
    "\n",
    "        data_tweets_recent = extract_100_tweets(query = \"{} -is:retweet\".format(query),\n",
    "                                                since_id=since_id,\n",
    "                                               key_twitter = bearer_token)\n",
    "        '''\n",
    "        time.sleep(1)\n",
    "\n",
    "        oldest_id = data_tweets_recent['meta']['oldest_id']\n",
    "\n",
    "        newest_id = data_tweets_recent['meta']['newest_id']\n",
    "\n",
    "        if df_concat_path:\n",
    "\n",
    "            df_data_tweets_old = pd.read_csv(df_concat_path)\n",
    "\n",
    "            df_data_tweets = pd.concat([data_tweets_recent,df_data_tweets_old])\n",
    "\n",
    "        else:\n",
    "\n",
    "            df_data_tweets = data_tweets_recent\n",
    "\n",
    "\n",
    "        name_file = \"{}/running/persist_tweets.csv\".format(folder)\n",
    "\n",
    "        # persist base\n",
    "                \n",
    "        df_data_tweets.to_csv(name_file,sep=\",\")\n",
    "        '''\n",
    "        \n",
    "        data = {\n",
    "                'data':data_tweets_recent[\"data\"],\n",
    "                'meta': {'newest_id': data_tweets_recent[\"meta\"][\"newest_id\"],\n",
    "                        'oldest_id': data_tweets_recent[\"meta\"][\"oldest_id\"],'query':query}\n",
    "                }       \n",
    "        \n",
    "    else:\n",
    "\n",
    "        \n",
    "        oldest_id = None\n",
    "\n",
    "        newest_id = None\n",
    "        \n",
    "        for i in tqdm(range(qnt_cycle)):\n",
    "        \n",
    "            \n",
    "            if i == 0:\n",
    "                \n",
    "                #extract the 100 tweets first\n",
    "                \n",
    "                if not until_id:\n",
    "            \n",
    "                    data_tweets = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=None,key_twitter = bearer_token)\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    data_tweets = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=until_id,key_twitter = bearer_token)\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                \n",
    "                df_data_tweets_temp = pd.DataFrame(data_tweets[\"data\"])\n",
    "                \n",
    "                #get the current date\n",
    "                \n",
    "                \n",
    "                oldest_id = data_tweets['meta']['oldest_id']\n",
    "\n",
    "                newest_id = data_tweets['meta']['newest_id']\n",
    "                \n",
    "                df_data_tweets = df_data_tweets_temp.copy()\n",
    "                \n",
    "                # name file\n",
    "                \n",
    "                #date_extraction_str = str(date_extraction).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "                \n",
    "                #name_file = \"{}/persist_tweets_{}_{}.csv\".format(folder,date_extraction_str,date_extraction_str)\n",
    "                \n",
    "                #name_file = \"{}/running/persist_tweets.csv\".format(folder)\n",
    "\n",
    "                # persist base\n",
    "                \n",
    "                #df_data_tweets.to_csv(name_file,sep=\",\")\n",
    "                \n",
    "        \n",
    "                \n",
    "            else:\n",
    "                \n",
    "                \n",
    "                #extract more 100 tweets older\n",
    "\n",
    "                data_tweets_temp = extract_100_tweets(query = \"{} -is:retweet\".format(query),until_id=oldest_id,key_twitter = bearer_token)\n",
    "                time.sleep(1)\n",
    "                \n",
    "                df_data_tweets_temp = pd.DataFrame(data_tweets_temp[\"data\"])\n",
    "                \n",
    "                \n",
    "                #get the current date\n",
    "                \n",
    "                \n",
    "                \n",
    "                oldest_id = data_tweets_temp['meta']['oldest_id']\n",
    "                \n",
    "                df_data_tweets = pd.concat([df_data_tweets,df_data_tweets_temp.copy()])\n",
    "\n",
    "                \n",
    "                df_data_tweets.reset_index(inplace=True,drop=True)\n",
    "                \n",
    "                \n",
    "                # remove old files\n",
    "                \n",
    "                #os.remove(name_file)\n",
    "                \n",
    "                \n",
    "                # name file\n",
    "                \n",
    "                #oldest_date_str = str(oldest_date).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "                \n",
    "                #date_extraction_str = str(date_extraction).replace(\".\",\"-\").replace(\":\",\"-\").replace(\" \",\"-\")\n",
    "                \n",
    "                \n",
    "                #name_file = \"{}/persist_tweets_{}_{}.csv\".format(folder,oldest_date_str,date_extraction_str)\n",
    "\n",
    "                #name_file = \"{}/running/persist_tweets.csv\".format(folder)\n",
    "                \n",
    "                # persist base\n",
    "                \n",
    "                #df_data_tweets.to_csv(name_file.format(folder),sep=\",\")\n",
    "                \n",
    "    \n",
    "        data = {\n",
    "                'data':df_data_tweets.to_dict(\"records\"),\n",
    "                'meta': {'newest_id': newest_id,\n",
    "                        'oldest_id': oldest_id,'query':query}\n",
    "                }\n",
    "\n",
    "    #status_system = {'df_concat_path':name_file,'newest_id':newest_id,'oldest_id':oldest_id,'query':query}\n",
    "\n",
    "    #f_status_system = open(\"{}/running/status_system.json\".format(folder), 'w')\n",
    "    #json.dump(status_system, f_status_system)\n",
    "\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "data_tweets_final = extract_many_tweets(qnt_cycle=1,folder=\"data_tweets\",query=\"globoplay\",bearer_token = BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_status_system = open(\"data_tweets.json\", 'w', encoding='utf8')\n",
    "\n",
    "\n",
    "json.dump(data_tweets_final, f_status_system, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'text': 'n tem nada d bom p fazer na netflix, amazon, globoplay, disney vou cancelar tudo, procuro as coisas pra ver e n acho',\n",
       "   'author_id': '1358960556246466560',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409795555334'},\n",
       "  {'text': '@dayane_v333 @HugoGloss Fica tudo no globoplay',\n",
       "   'author_id': '1274914698358636547',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409258684418'},\n",
       "  {'text': 'perdi o começo do no limite agora to esperando acabar pra ver a integra no globoplay https://t.co/WcbYuoHQAs',\n",
       "   'author_id': '1308267423813038080',\n",
       "   'created_at': '2021-05-12T02:51:18.000Z',\n",
       "   'id': '1392311390937960451'},\n",
       "  {'text': 'A entrevista com o eliminado vai ser ao vivo no @Globoplay com a @anaclaraac assim que acabar i programa, igual o BBB  #NoLimite',\n",
       "   'author_id': '104565437',\n",
       "   'created_at': '2021-05-12T02:51:13.000Z',\n",
       "   'id': '1392311370344013825'},\n",
       "  {'text': 'Minha gente, esse mahmoud é perigosíssimo #NoLimite #globoplay #redeglobo',\n",
       "   'author_id': '1391228459431743502',\n",
       "   'created_at': '2021-05-12T02:51:08.000Z',\n",
       "   'id': '1392311349934440449'},\n",
       "  {'text': '@dearmaria44 Globoplay respira por aparelhos! 🙏😰',\n",
       "   'author_id': '819736443706806272',\n",
       "   'created_at': '2021-05-12T02:51:05.000Z',\n",
       "   'id': '1392311336537923587'},\n",
       "  {'text': 'A cada comercial entra a propaganda de um filme num serviço de streaming diferente.\\n\\nEsperta é a Globo que lucra de todos e investe no Globoplay.\\n\\n#NoLimite',\n",
       "   'author_id': '1705442492',\n",
       "   'created_at': '2021-05-12T02:51:03.000Z',\n",
       "   'id': '1392311330959462403'},\n",
       "  {'text': '@Maaniana_ Pode assistir a gravação na Globoplay no dia seguinte ué',\n",
       "   'author_id': '794560160727441413',\n",
       "   'created_at': '2021-05-12T02:50:58.000Z',\n",
       "   'id': '1392311306611568644'},\n",
       "  {'text': 'me expliquem como vocês se desapegaram do bbb tão rápido pois sinceramente eu ainda fico aqui entrando no app do globoplay a cada tempinho livre antes de cair a ficha que o trem acabou',\n",
       "   'author_id': '2923469693',\n",
       "   'created_at': '2021-05-12T02:50:48.000Z',\n",
       "   'id': '1392311264932728833'},\n",
       "  {'text': '@globoplay não acredito que você vai deixar a @PrimeVideoBR exibir a 4 temporada de The handmaid’s tale primeiro....',\n",
       "   'author_id': '229861024',\n",
       "   'created_at': '2021-05-12T02:50:39.000Z',\n",
       "   'id': '1392311229058846725'},\n",
       "  {'text': 'n tem nada d bom p fazer na netflix, amazon, globoplay, disney vou cancelar tudo, procuro as coisas pra ver e n acho',\n",
       "   'author_id': '1358960556246466560',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409795555334'},\n",
       "  {'text': '@dayane_v333 @HugoGloss Fica tudo no globoplay',\n",
       "   'author_id': '1274914698358636547',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409258684418'},\n",
       "  {'text': 'perdi o começo do no limite agora to esperando acabar pra ver a integra no globoplay https://t.co/WcbYuoHQAs',\n",
       "   'author_id': '1308267423813038080',\n",
       "   'created_at': '2021-05-12T02:51:18.000Z',\n",
       "   'id': '1392311390937960451'},\n",
       "  {'text': 'A entrevista com o eliminado vai ser ao vivo no @Globoplay com a @anaclaraac assim que acabar i programa, igual o BBB  #NoLimite',\n",
       "   'author_id': '104565437',\n",
       "   'created_at': '2021-05-12T02:51:13.000Z',\n",
       "   'id': '1392311370344013825'},\n",
       "  {'text': 'Minha gente, esse mahmoud é perigosíssimo #NoLimite #globoplay #redeglobo',\n",
       "   'author_id': '1391228459431743502',\n",
       "   'created_at': '2021-05-12T02:51:08.000Z',\n",
       "   'id': '1392311349934440449'},\n",
       "  {'text': '@dearmaria44 Globoplay respira por aparelhos! 🙏😰',\n",
       "   'author_id': '819736443706806272',\n",
       "   'created_at': '2021-05-12T02:51:05.000Z',\n",
       "   'id': '1392311336537923587'},\n",
       "  {'text': 'A cada comercial entra a propaganda de um filme num serviço de streaming diferente.\\n\\nEsperta é a Globo que lucra de todos e investe no Globoplay.\\n\\n#NoLimite',\n",
       "   'author_id': '1705442492',\n",
       "   'created_at': '2021-05-12T02:51:03.000Z',\n",
       "   'id': '1392311330959462403'},\n",
       "  {'text': '@Maaniana_ Pode assistir a gravação na Globoplay no dia seguinte ué',\n",
       "   'author_id': '794560160727441413',\n",
       "   'created_at': '2021-05-12T02:50:58.000Z',\n",
       "   'id': '1392311306611568644'},\n",
       "  {'text': 'me expliquem como vocês se desapegaram do bbb tão rápido pois sinceramente eu ainda fico aqui entrando no app do globoplay a cada tempinho livre antes de cair a ficha que o trem acabou',\n",
       "   'author_id': '2923469693',\n",
       "   'created_at': '2021-05-12T02:50:48.000Z',\n",
       "   'id': '1392311264932728833'},\n",
       "  {'text': '@globoplay não acredito que você vai deixar a @PrimeVideoBR exibir a 4 temporada de The handmaid’s tale primeiro....',\n",
       "   'author_id': '229861024',\n",
       "   'created_at': '2021-05-12T02:50:39.000Z',\n",
       "   'id': '1392311229058846725'},\n",
       "  {'text': 'n tem nada d bom p fazer na netflix, amazon, globoplay, disney vou cancelar tudo, procuro as coisas pra ver e n acho',\n",
       "   'author_id': '1358960556246466560',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409795555334'},\n",
       "  {'text': '@dayane_v333 @HugoGloss Fica tudo no globoplay',\n",
       "   'author_id': '1274914698358636547',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409258684418'},\n",
       "  {'text': 'perdi o começo do no limite agora to esperando acabar pra ver a integra no globoplay https://t.co/WcbYuoHQAs',\n",
       "   'author_id': '1308267423813038080',\n",
       "   'created_at': '2021-05-12T02:51:18.000Z',\n",
       "   'id': '1392311390937960451'},\n",
       "  {'text': 'A entrevista com o eliminado vai ser ao vivo no @Globoplay com a @anaclaraac assim que acabar i programa, igual o BBB  #NoLimite',\n",
       "   'author_id': '104565437',\n",
       "   'created_at': '2021-05-12T02:51:13.000Z',\n",
       "   'id': '1392311370344013825'},\n",
       "  {'text': 'Minha gente, esse mahmoud é perigosíssimo #NoLimite #globoplay #redeglobo',\n",
       "   'author_id': '1391228459431743502',\n",
       "   'created_at': '2021-05-12T02:51:08.000Z',\n",
       "   'id': '1392311349934440449'},\n",
       "  {'text': '@dearmaria44 Globoplay respira por aparelhos! 🙏😰',\n",
       "   'author_id': '819736443706806272',\n",
       "   'created_at': '2021-05-12T02:51:05.000Z',\n",
       "   'id': '1392311336537923587'},\n",
       "  {'text': 'A cada comercial entra a propaganda de um filme num serviço de streaming diferente.\\n\\nEsperta é a Globo que lucra de todos e investe no Globoplay.\\n\\n#NoLimite',\n",
       "   'author_id': '1705442492',\n",
       "   'created_at': '2021-05-12T02:51:03.000Z',\n",
       "   'id': '1392311330959462403'},\n",
       "  {'text': '@Maaniana_ Pode assistir a gravação na Globoplay no dia seguinte ué',\n",
       "   'author_id': '794560160727441413',\n",
       "   'created_at': '2021-05-12T02:50:58.000Z',\n",
       "   'id': '1392311306611568644'},\n",
       "  {'text': 'me expliquem como vocês se desapegaram do bbb tão rápido pois sinceramente eu ainda fico aqui entrando no app do globoplay a cada tempinho livre antes de cair a ficha que o trem acabou',\n",
       "   'author_id': '2923469693',\n",
       "   'created_at': '2021-05-12T02:50:48.000Z',\n",
       "   'id': '1392311264932728833'},\n",
       "  {'text': '@globoplay não acredito que você vai deixar a @PrimeVideoBR exibir a 4 temporada de The handmaid’s tale primeiro....',\n",
       "   'author_id': '229861024',\n",
       "   'created_at': '2021-05-12T02:50:39.000Z',\n",
       "   'id': '1392311229058846725'},\n",
       "  {'text': '@nylloca Na globo mesmo depois eles devem colocar o episódio no globoplay',\n",
       "   'author_id': '3557827408',\n",
       "   'created_at': '2021-05-12T02:51:28.000Z',\n",
       "   'id': '1392311435888312320'},\n",
       "  {'text': 'n tem nada d bom p fazer na netflix, amazon, globoplay, disney vou cancelar tudo, procuro as coisas pra ver e n acho',\n",
       "   'author_id': '1358960556246466560',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409795555334'},\n",
       "  {'text': '@dayane_v333 @HugoGloss Fica tudo no globoplay',\n",
       "   'author_id': '1274914698358636547',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409258684418'},\n",
       "  {'text': 'perdi o começo do no limite agora to esperando acabar pra ver a integra no globoplay https://t.co/WcbYuoHQAs',\n",
       "   'author_id': '1308267423813038080',\n",
       "   'created_at': '2021-05-12T02:51:18.000Z',\n",
       "   'id': '1392311390937960451'},\n",
       "  {'text': 'A entrevista com o eliminado vai ser ao vivo no @Globoplay com a @anaclaraac assim que acabar i programa, igual o BBB  #NoLimite',\n",
       "   'author_id': '104565437',\n",
       "   'created_at': '2021-05-12T02:51:13.000Z',\n",
       "   'id': '1392311370344013825'},\n",
       "  {'text': 'Minha gente, esse mahmoud é perigosíssimo #NoLimite #globoplay #redeglobo',\n",
       "   'author_id': '1391228459431743502',\n",
       "   'created_at': '2021-05-12T02:51:08.000Z',\n",
       "   'id': '1392311349934440449'},\n",
       "  {'text': '@dearmaria44 Globoplay respira por aparelhos! 🙏😰',\n",
       "   'author_id': '819736443706806272',\n",
       "   'created_at': '2021-05-12T02:51:05.000Z',\n",
       "   'id': '1392311336537923587'},\n",
       "  {'text': 'A cada comercial entra a propaganda de um filme num serviço de streaming diferente.\\n\\nEsperta é a Globo que lucra de todos e investe no Globoplay.\\n\\n#NoLimite',\n",
       "   'author_id': '1705442492',\n",
       "   'created_at': '2021-05-12T02:51:03.000Z',\n",
       "   'id': '1392311330959462403'},\n",
       "  {'text': '@Maaniana_ Pode assistir a gravação na Globoplay no dia seguinte ué',\n",
       "   'author_id': '794560160727441413',\n",
       "   'created_at': '2021-05-12T02:50:58.000Z',\n",
       "   'id': '1392311306611568644'},\n",
       "  {'text': 'me expliquem como vocês se desapegaram do bbb tão rápido pois sinceramente eu ainda fico aqui entrando no app do globoplay a cada tempinho livre antes de cair a ficha que o trem acabou',\n",
       "   'author_id': '2923469693',\n",
       "   'created_at': '2021-05-12T02:50:48.000Z',\n",
       "   'id': '1392311264932728833'},\n",
       "  {'text': '@nylloca Na globo mesmo depois eles devem colocar o episódio no globoplay',\n",
       "   'author_id': '3557827408',\n",
       "   'created_at': '2021-05-12T02:51:28.000Z',\n",
       "   'id': '1392311435888312320'},\n",
       "  {'text': 'n tem nada d bom p fazer na netflix, amazon, globoplay, disney vou cancelar tudo, procuro as coisas pra ver e n acho',\n",
       "   'author_id': '1358960556246466560',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409795555334'},\n",
       "  {'text': '@dayane_v333 @HugoGloss Fica tudo no globoplay',\n",
       "   'author_id': '1274914698358636547',\n",
       "   'created_at': '2021-05-12T02:51:22.000Z',\n",
       "   'id': '1392311409258684418'},\n",
       "  {'text': 'perdi o começo do no limite agora to esperando acabar pra ver a integra no globoplay https://t.co/WcbYuoHQAs',\n",
       "   'author_id': '1308267423813038080',\n",
       "   'created_at': '2021-05-12T02:51:18.000Z',\n",
       "   'id': '1392311390937960451'},\n",
       "  {'text': 'A entrevista com o eliminado vai ser ao vivo no @Globoplay com a @anaclaraac assim que acabar i programa, igual o BBB  #NoLimite',\n",
       "   'author_id': '104565437',\n",
       "   'created_at': '2021-05-12T02:51:13.000Z',\n",
       "   'id': '1392311370344013825'},\n",
       "  {'text': 'Minha gente, esse mahmoud é perigosíssimo #NoLimite #globoplay #redeglobo',\n",
       "   'author_id': '1391228459431743502',\n",
       "   'created_at': '2021-05-12T02:51:08.000Z',\n",
       "   'id': '1392311349934440449'},\n",
       "  {'text': '@dearmaria44 Globoplay respira por aparelhos! 🙏😰',\n",
       "   'author_id': '819736443706806272',\n",
       "   'created_at': '2021-05-12T02:51:05.000Z',\n",
       "   'id': '1392311336537923587'},\n",
       "  {'text': 'A cada comercial entra a propaganda de um filme num serviço de streaming diferente.\\n\\nEsperta é a Globo que lucra de todos e investe no Globoplay.\\n\\n#NoLimite',\n",
       "   'author_id': '1705442492',\n",
       "   'created_at': '2021-05-12T02:51:03.000Z',\n",
       "   'id': '1392311330959462403'},\n",
       "  {'text': '@Maaniana_ Pode assistir a gravação na Globoplay no dia seguinte ué',\n",
       "   'author_id': '794560160727441413',\n",
       "   'created_at': '2021-05-12T02:50:58.000Z',\n",
       "   'id': '1392311306611568644'},\n",
       "  {'text': 'me expliquem como vocês se desapegaram do bbb tão rápido pois sinceramente eu ainda fico aqui entrando no app do globoplay a cada tempinho livre antes de cair a ficha que o trem acabou',\n",
       "   'author_id': '2923469693',\n",
       "   'created_at': '2021-05-12T02:50:48.000Z',\n",
       "   'id': '1392311264932728833'}],\n",
       " 'meta': {'newest_id': '1392311409795555334',\n",
       "  'oldest_id': '1392311264932728833',\n",
       "  'query': 'globoplay'}}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@globoplay será que a senhora poderia por favor assinar um contrato com a dona @amazonBR para que eu possa assistir vocês na minha tv? Obrigada, de nada!\\U0001f972'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets_final[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final = extract_many_tweets(qnt_cycle=1,folder=\"data_tweets\",query=\"globoplay\",bearer_token = BEARER_TOKEN,since_id=\"1392305680640585733\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data_tweets_final[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-05-16T21:00:43.000Z'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"created_at\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 5, 16, 21, 0, 43)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "date_time_str = '2021-05-16T21:00:43.000Z'.replace(\"T\",\" \").replace(\"Z\",\"\")\n",
    "date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "date_time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2021-05-16T21:00:43.000Z\n",
       "1    2021-05-16T21:00:30.000Z\n",
       "2    2021-05-16T21:00:00.000Z\n",
       "3    2021-05-16T20:59:28.000Z\n",
       "4    2021-05-16T20:59:10.000Z\n",
       "5    2021-05-16T20:57:56.000Z\n",
       "6    2021-05-16T20:57:21.000Z\n",
       "7    2021-05-16T20:56:15.000Z\n",
       "8    2021-05-16T20:56:06.000Z\n",
       "9    2021-05-16T20:56:02.000Z\n",
       "Name: created_at, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"created_at\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1392301970585792515',\n",
       "  'created_at': '2021-05-12T02:13:52.000Z',\n",
       "  'author_id': '1293754876825153537',\n",
       "  'text': 'Reprodução/Globoplay https://t.co/I1j5RcqZTP'},\n",
       " {'id': '1392301962876555265',\n",
       "  'created_at': '2021-05-12T02:13:50.000Z',\n",
       "  'author_id': '382316896',\n",
       "  'text': 'a dona da globoplay tendo patrocinio da amazon prime pro programa e botando comercial da netflix\\n\\nunião entre streamings'},\n",
       " {'id': '1392301955024838658',\n",
       "  'created_at': '2021-05-12T02:13:48.000Z',\n",
       "  'author_id': '1285377843581661186',\n",
       "  'text': 'globoplay eh muito merda eu PAGO esse cu mas to vendo a programação por um daqueles canal que passa a programação ao vivo pelo youtube'},\n",
       " {'id': '1392301932618866690',\n",
       "  'created_at': '2021-05-12T02:13:43.000Z',\n",
       "  'author_id': '90660978',\n",
       "  'text': 'Alguém me explica pq não tá passando no limite na Globoplay?'},\n",
       " {'id': '1392301908640083978',\n",
       "  'created_at': '2021-05-12T02:13:37.000Z',\n",
       "  'author_id': '1349167491386372097',\n",
       "  'text': 'A tribo calango acaba de vencer a segunda prova... gostaram?\\n\\n#nolimite #prontofaleynolimite \\nImagem/reprodução: @nolimite @globoplay @tvglobo https://t.co/0N97bVRn4r'},\n",
       " {'id': '1392301896916930566',\n",
       "  'created_at': '2021-05-12T02:13:34.000Z',\n",
       "  'author_id': '3240847114',\n",
       "  'text': '@Nanda_Loren @globoplay Eles passaram no ao vivo sim'},\n",
       " {'id': '1392301894425616387',\n",
       "  'created_at': '2021-05-12T02:13:34.000Z',\n",
       "  'author_id': '2157992258',\n",
       "  'text': 'Olha a concorrente ai, alo globoplay'},\n",
       " {'id': '1392301873118466051',\n",
       "  'created_at': '2021-05-12T02:13:28.000Z',\n",
       "  'author_id': '999809364',\n",
       "  'text': 'Pq o Globoplay ao vivo não funciona na Bahia? Affff'},\n",
       " {'id': '1392301831200591875',\n",
       "  'created_at': '2021-05-12T02:13:18.000Z',\n",
       "  'author_id': '53046167',\n",
       "  'text': '@boninho @tvglobo @NoLimite Cadê para a Globoplay Internacional?'},\n",
       " {'id': '1392301812628312069',\n",
       "  'created_at': '2021-05-12T02:13:14.000Z',\n",
       "  'author_id': '74212028',\n",
       "  'text': 'E a internet que acabou aqui em casa, não voltou e eu só tenho globoplay pra assistir TV kkkkkk e hj é a estreia de No Limite.'}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '1392301970585792515',\n",
       "   'created_at': '2021-05-12T02:13:52.000Z',\n",
       "   'author_id': '1293754876825153537',\n",
       "   'text': 'Reprodução/Globoplay https://t.co/I1j5RcqZTP'},\n",
       "  {'id': '1392301962876555265',\n",
       "   'created_at': '2021-05-12T02:13:50.000Z',\n",
       "   'author_id': '382316896',\n",
       "   'text': 'a dona da globoplay tendo patrocinio da amazon prime pro programa e botando comercial da netflix\\n\\nunião entre streamings'},\n",
       "  {'id': '1392301955024838658',\n",
       "   'created_at': '2021-05-12T02:13:48.000Z',\n",
       "   'author_id': '1285377843581661186',\n",
       "   'text': 'globoplay eh muito merda eu PAGO esse cu mas to vendo a programação por um daqueles canal que passa a programação ao vivo pelo youtube'},\n",
       "  {'id': '1392301932618866690',\n",
       "   'created_at': '2021-05-12T02:13:43.000Z',\n",
       "   'author_id': '90660978',\n",
       "   'text': 'Alguém me explica pq não tá passando no limite na Globoplay?'},\n",
       "  {'id': '1392301908640083978',\n",
       "   'created_at': '2021-05-12T02:13:37.000Z',\n",
       "   'author_id': '1349167491386372097',\n",
       "   'text': 'A tribo calango acaba de vencer a segunda prova... gostaram?\\n\\n#nolimite #prontofaleynolimite \\nImagem/reprodução: @nolimite @globoplay @tvglobo https://t.co/0N97bVRn4r'},\n",
       "  {'id': '1392301896916930566',\n",
       "   'created_at': '2021-05-12T02:13:34.000Z',\n",
       "   'author_id': '3240847114',\n",
       "   'text': '@Nanda_Loren @globoplay Eles passaram no ao vivo sim'},\n",
       "  {'id': '1392301894425616387',\n",
       "   'created_at': '2021-05-12T02:13:34.000Z',\n",
       "   'author_id': '2157992258',\n",
       "   'text': 'Olha a concorrente ai, alo globoplay'},\n",
       "  {'id': '1392301873118466051',\n",
       "   'created_at': '2021-05-12T02:13:28.000Z',\n",
       "   'author_id': '999809364',\n",
       "   'text': 'Pq o Globoplay ao vivo não funciona na Bahia? Affff'},\n",
       "  {'id': '1392301831200591875',\n",
       "   'created_at': '2021-05-12T02:13:18.000Z',\n",
       "   'author_id': '53046167',\n",
       "   'text': '@boninho @tvglobo @NoLimite Cadê para a Globoplay Internacional?'},\n",
       "  {'id': '1392301812628312069',\n",
       "   'created_at': '2021-05-12T02:13:14.000Z',\n",
       "   'author_id': '74212028',\n",
       "   'text': 'E a internet que acabou aqui em casa, não voltou e eu só tenho globoplay pra assistir TV kkkkkk e hj é a estreia de No Limite.'}],\n",
       " 'meta': {'newest_id': '1392301970585792515',\n",
       "  'oldest_id': '1392301812628312069',\n",
       "  'result_count': 10,\n",
       "  'next_token': 'b26v89c19zqg8o3foswrej4zf4159x61qmxgk6byxlrwd'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1392301970585792515</td>\n",
       "      <td>2021-05-12T02:13:52.000Z</td>\n",
       "      <td>1293754876825153537</td>\n",
       "      <td>Reprodução/Globoplay https://t.co/I1j5RcqZTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1392301962876555265</td>\n",
       "      <td>2021-05-12T02:13:50.000Z</td>\n",
       "      <td>382316896</td>\n",
       "      <td>a dona da globoplay tendo patrocinio da amazon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1392301955024838658</td>\n",
       "      <td>2021-05-12T02:13:48.000Z</td>\n",
       "      <td>1285377843581661186</td>\n",
       "      <td>globoplay eh muito merda eu PAGO esse cu mas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1392301932618866690</td>\n",
       "      <td>2021-05-12T02:13:43.000Z</td>\n",
       "      <td>90660978</td>\n",
       "      <td>Alguém me explica pq não tá passando no limite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1392301908640083978</td>\n",
       "      <td>2021-05-12T02:13:37.000Z</td>\n",
       "      <td>1349167491386372097</td>\n",
       "      <td>A tribo calango acaba de vencer a segunda prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1392301896916930566</td>\n",
       "      <td>2021-05-12T02:13:34.000Z</td>\n",
       "      <td>3240847114</td>\n",
       "      <td>@Nanda_Loren @globoplay Eles passaram no ao vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1392301894425616387</td>\n",
       "      <td>2021-05-12T02:13:34.000Z</td>\n",
       "      <td>2157992258</td>\n",
       "      <td>Olha a concorrente ai, alo globoplay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1392301873118466051</td>\n",
       "      <td>2021-05-12T02:13:28.000Z</td>\n",
       "      <td>999809364</td>\n",
       "      <td>Pq o Globoplay ao vivo não funciona na Bahia? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1392301831200591875</td>\n",
       "      <td>2021-05-12T02:13:18.000Z</td>\n",
       "      <td>53046167</td>\n",
       "      <td>@boninho @tvglobo @NoLimite Cadê para a Globop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1392301812628312069</td>\n",
       "      <td>2021-05-12T02:13:14.000Z</td>\n",
       "      <td>74212028</td>\n",
       "      <td>E a internet que acabou aqui em casa, não volt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                created_at            author_id  \\\n",
       "0  1392301970585792515  2021-05-12T02:13:52.000Z  1293754876825153537   \n",
       "1  1392301962876555265  2021-05-12T02:13:50.000Z            382316896   \n",
       "2  1392301955024838658  2021-05-12T02:13:48.000Z  1285377843581661186   \n",
       "3  1392301932618866690  2021-05-12T02:13:43.000Z             90660978   \n",
       "4  1392301908640083978  2021-05-12T02:13:37.000Z  1349167491386372097   \n",
       "5  1392301896916930566  2021-05-12T02:13:34.000Z           3240847114   \n",
       "6  1392301894425616387  2021-05-12T02:13:34.000Z           2157992258   \n",
       "7  1392301873118466051  2021-05-12T02:13:28.000Z            999809364   \n",
       "8  1392301831200591875  2021-05-12T02:13:18.000Z             53046167   \n",
       "9  1392301812628312069  2021-05-12T02:13:14.000Z             74212028   \n",
       "\n",
       "                                                text  \n",
       "0       Reprodução/Globoplay https://t.co/I1j5RcqZTP  \n",
       "1  a dona da globoplay tendo patrocinio da amazon...  \n",
       "2  globoplay eh muito merda eu PAGO esse cu mas t...  \n",
       "3  Alguém me explica pq não tá passando no limite...  \n",
       "4  A tribo calango acaba de vencer a segunda prov...  \n",
       "5  @Nanda_Loren @globoplay Eles passaram no ao vi...  \n",
       "6               Olha a concorrente ai, alo globoplay  \n",
       "7  Pq o Globoplay ao vivo não funciona na Bahia? ...  \n",
       "8  @boninho @tvglobo @NoLimite Cadê para a Globop...  \n",
       "9  E a internet que acabou aqui em casa, não volt...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@globoplay Eu ainda não acredito que vocês fizeram isso... \\n\\nPelo amor de Deus. \\n\\nEu não vou perder meu tempo. \\nE pra mim tomara que perda o investimento. \\n\\nSó fez maldade... maldosa mesmo.. a fama dela com relacionamentos com pessoas já era ruim aqui fora.. ai mudou do nada. NAO ACREDITO'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tweets_final[\"text\"][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final[\"text_clean\"] = data_tweets_final[\"text\"].apply(lambda x: text_cleaner(text = x,stop_words_domain =None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final[[\"text\",\"text_clean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(,data_tweets_final[\"text\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final[\"text\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'https://t.co/HaKu8sRLkX Melhor pessoa desse bbb '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r\"\\shttps([a-zA-Zà-úÀ-Ú0-9]|[-()\\\"#/@;:<>{}`+=~|.!?,])+$|^https([a-zA-Zà-úÀ-Ú0-9]|[-()\\\"#/@;:<>{}`+=~|.!?,])+\\s|\\shttps([a-zA-Zà-úÀ-Ú0-9]|[-()\\\"#/@;:<>{}`+=~|.!?,])+\\s\",\" \",test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(r\"\\shttps([a-zA-Zà-úÀ-Ú0-9]|[^a-zA-Zà-úÀ-Ú0-9])+$|^https([a-zA-Zà-úÀ-Ú0-9]|[^a-zA-Zà-úÀ-Ú0-9])+\\s|\\shttps([a-zA-Zà-úÀ-Ú0-9]|[^a-zA-Zà-úÀ-Ú0-9])+\\s\",\" \",test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https([a-zA-Zà-úÀ-Ú0-9]|[^a-zA-Zà-úÀ-Ú0-9])+\\s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_calc_histogram(x,initial_interval, final_interval,n_bins,indice=False):\n",
    "\n",
    "    interval = np.linspace(initial_interval, final_interval, num=n_bins)\n",
    "\n",
    "    for j,i in enumerate(interval):\n",
    "\n",
    "\n",
    "        if i == interval[len(interval)-1]:\n",
    "\n",
    "            if x>=i:\n",
    "\n",
    "                \n",
    "                if indice:\n",
    "\n",
    "                    return j\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    return \"{}<\".format(x)\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            if x>=i and x<interval[j+1]:\n",
    "\n",
    "                inicial = round(i, 1)\n",
    "\n",
    "                final = round(interval[j+1],1)\n",
    "\n",
    "            \n",
    "                if indice:\n",
    "\n",
    "                    return j\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    return \"[{},{})\".format(inicial,final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de uma coluna com os textos sem repetição de palavras para ser utilizado na análise exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['text_unique_words'] = data_tweets_final['text'].apply(lambda x: convert_text_to_no_repeat_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo Número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['number_tokens'] = data_tweets_final['text'].apply(lambda x: calculate_number_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo Número de diferentes tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['number_diferent_tokens'] = data_tweets_final['text'].apply(lambda x: calculate_number_diferent_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máximo número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = data_tweets_final[\"number_tokens\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final[\"number_tokens\"].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mínimo número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = data_tweets_final[\"number_tokens\"].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dados para o histograma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['bins'] = data_tweets_final['number_tokens'].apply(lambda x: function_to_calc_histogram(x,initial_interval = min_count, final_interval = max_count,n_bins = 10,indice=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tweets_final['indices_bins'] = data_tweets_final['number_tokens'].apply(lambda x: function_to_calc_histogram(x,initial_interval = min_count, final_interval = max_count,n_bins = 10,indice=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram = data_tweets_final.groupby([\"bins\",\"indices_bins\"]).sum().sort_values(by=[\"indices_bins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram[\"bins\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_histogram[\"number_tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 MEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_mean = plot_bar_count_words(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='MEAN',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_mean[\"MEAN\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_mean[\"WORDS\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 SUM docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum = plot_bar_count_words(text_column='text_unique_words',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='SUM',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum[\"P_DOCS\"] =  df_report_sum[\"SUM\"]/len(data_tweets_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 SUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum_docs = plot_bar_count_words(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='SUM',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_sum_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 MEAN TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_mean = plot_bar_tf_idf(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='MEAN',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF top 10 MAX TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_max = plot_bar_tf_idf(text_column='text',\n",
    "                                                dataframe=data_tweets_final,\n",
    "                                                metric='MAX',top=10,return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report_tfidf_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
